{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "df = pd.read_csv('online_shoppers_intention.csv', na_values=['NA', 'null', '', 'NULL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Getting our first look at the Dataset using the describe function on our dataset\n",
    "df_cpy = df.copy()\n",
    "df.head()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "# Feature groups that have similar representation\n",
    "num_pages_visited_features = ['Administrative', 'Informational', 'ProductRelated']\n",
    "total_duration_page_features = ['Administrative_Duration', 'Informational_Duration', 'ProductRelated_Duration']\n",
    "\n",
    "# Calculate means for number of visits for every page type\n",
    "mean_num_pages_visited = df[num_pages_visited_features].mean()\n",
    "\n",
    "# Show above calculations in a bar plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=num_pages_visited_features, y=mean_num_pages_visited, name=\"mean per page category\"))\n",
    "fig.show()\n",
    "\n",
    "# Calculate average duration per page for every page type\n",
    "total_duration = df[total_duration_page_features].sum()\n",
    "total_pages_visited = df[num_pages_visited_features].sum().astype(float)\n",
    "avg_duration_per_page = total_duration.values / total_pages_visited.values\n",
    "\n",
    "# Show above calculations in a barplot\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Bar(x=num_pages_visited_features, y=avg_duration_per_page, name=\"average duration per page\"))\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Comparing characteristics of the dataset for the two different types of users\n",
    "\n",
    "# Filter dataset on browser == 13 and != 13\n",
    "loc_df_browsers_13 = df.loc[df['Browser'] == 13]\n",
    "loc_df_browsers_not_13 = df.loc[df['Browser'] != 13]\n",
    "\n",
    "# Calculate means for number of visits for each page type \n",
    "mean_browsers_13_ints = loc_df_browsers_13[num_pages_visited_features].mean()\n",
    "mean_browsers_not_13_ints = loc_df_browsers_not_13[num_pages_visited_features].mean()\n",
    "\n",
    "# Show above calculations in a barplot in which we can compare browser ==13 and !=13\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=num_pages_visited_features, y=mean_browsers_13_ints, name='Browser == 13'))\n",
    "fig.add_trace(go.Bar(x=num_pages_visited_features, y=mean_browsers_not_13_ints, name='Browser != 13'))\n",
    "fig.update_layout(title='Comparison of Feature Means: Browser == 13 vs Browser != 13',xaxis_title='Page types',yaxis_title='Mean Value')\n",
    "fig.show()\n",
    "\n",
    "# Calculate average duration per page for every page type for browser==13\n",
    "total_duration_13 = loc_df_browsers_13[total_duration_page_features].sum()\n",
    "total_pages_visited_13 = loc_df_browsers_13[num_pages_visited_features].sum().astype(float)\n",
    "avg_duration_per_page_13 = total_duration_13.values / total_pages_visited_13.values\n",
    "\n",
    "# Calculate average duration per page for every page type for browser!=13\n",
    "total_duration_not_13 = loc_df_browsers_not_13[total_duration_page_features].sum()\n",
    "total_pages_visited_not_13 = loc_df_browsers_not_13[num_pages_visited_features].sum().astype(float)\n",
    "avg_duration_per_page_not_13 = total_duration_not_13.values / total_pages_visited_not_13.values\n",
    "\n",
    "# Show above two calculations in a barplot in which we can compare browser ==13 and !=13\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Bar(x=num_pages_visited_features, y=avg_duration_per_page_13, name=\"Browser == 13\"))\n",
    "fig2.add_trace(go.Bar(x=num_pages_visited_features, y=avg_duration_per_page_not_13, name=\"Browser != 13\"))\n",
    "fig2.update_layout(title='Average duration per page visit: Browser == 13 vs Browser != 13',\n",
    "    xaxis_title='Page types',\n",
    "    yaxis_title='Average duration per page visit',\n",
    "    yaxis=dict(range=[0,100]),\n",
    ")\n",
    "fig2.show()\n",
    "\n",
    "# Calculate percentages for when revenue is true for both browser ==13 and !=13\n",
    "sum_revenue_13 = loc_df_browsers_13.loc[loc_df_browsers_13['Revenue']]\n",
    "percentage_revenue_13 = (len(sum_revenue_13) / len(loc_df_browsers_13)) * 100\n",
    "\n",
    "sum_revenue_not_13 = loc_df_browsers_not_13.loc[loc_df_browsers_not_13['Revenue']]\n",
    "percentage_revenue_not_13 = (len(sum_revenue_not_13) / len(loc_df_browsers_not_13)) * 100\n",
    "\n",
    "# Show above percentage calculations in a barplot to compare browser ==13 and !=13\n",
    "fig3 = go.Figure()\n",
    "fig3.add_trace(go.Bar(x=['Browser == 13'], y=[percentage_revenue_13], name='Browser == 13'))\n",
    "fig3.add_trace(go.Bar(x=['Browser != 13'], y=[percentage_revenue_not_13], name='Browser != 13'))\n",
    "fig3.update_layout(title='Revenue Conversion Rate: Browser == 13 vs Browser != 13',\n",
    "    xaxis_title='Browser Group',\n",
    "    yaxis_title='% of Users with Revenue == True',\n",
    "    yaxis=dict(range=[0,100]),\n",
    ")\n",
    "fig3.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms (frequencies)\n",
    "month_h = px.histogram(df, x='Month')\n",
    "os_h = px.histogram(df, x='OperatingSystems')\n",
    "browser_h = px.histogram(df, x='Browser')\n",
    "region_h = px.histogram(df, x='Region')\n",
    "traffic_type_h = px.histogram(df, x='TrafficType')\n",
    "visitor_type_h = px.histogram(df, x='VisitorType')\n",
    "weekend_h = px.histogram(df, x='Weekend')\n",
    "revenue_h = px.histogram(df, x='Revenue')\n",
    "\n",
    "month_h.show()\n",
    "os_h.show()\n",
    "browser_h.show()\n",
    "region_h.show()\n",
    "traffic_type_h.show()\n",
    "visitor_type_h.show()\n",
    "weekend_h.show()\n",
    "revenue_h.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no january and april. \n",
    "What means other?\n",
    "Week day 1892.4 visitors on average and 1434 visitors in the weekend. A differnce of 458.4 visitors per day on average. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do percentages, for the reveneu, and let's show also like a number of visitors per day.  \n",
    "\n",
    "os_h = px.histogram(df, x='OperatingSystems')\n",
    "browser_h = px.histogram(df, x='Browser')\n",
    "region_h = px.histogram(df, x='Region')\n",
    "traffic_type_h = px.histogram(df, x='TrafficType') \n",
    "What the different categories mean?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    'Administrative',\n",
    "    'Administrative_Duration',\n",
    "    'Informational',\n",
    "    'Informational_Duration',\n",
    "    'ProductRelated',\n",
    "    'ProductRelated_Duration',\n",
    "    'BounceRates',\n",
    "    'ExitRates',\n",
    "    'PageValues',\n",
    "    'SpecialDay'\n",
    "]\n",
    "for var in variables:\n",
    "    fig = px.box(df, y=var, title=f'Boxplot of {var}')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cp = df.copy()\n",
    "df_cp.dropna(inplace=True)\n",
    "\n",
    "# 3.2\n",
    "# Sinlge column normalization\n",
    "def lin_norm(val, col):\n",
    "    val = val[[col]]\n",
    "    max_val = val.max()\n",
    "    min_val = val.min()\n",
    "    return (val - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to normalize\n",
    "variables = [\n",
    "    'Administrative',\n",
    "    'Administrative_Duration',\n",
    "    'Informational',\n",
    "    'Informational_Duration',\n",
    "    'ProductRelated',\n",
    "    'ProductRelated_Duration',\n",
    "    'BounceRates',\n",
    "    'ExitRates',\n",
    "    'PageValues',\n",
    "    'SpecialDay'\n",
    "]\n",
    "\n",
    "# Copy the dataframe and drop missing values\n",
    "df_cp = df.copy()\n",
    "df_cp.dropna(inplace=True)\n",
    "\n",
    "# Sinlge column normalization\n",
    "def lin_norm(df, col):\n",
    "    max_val = df[col].max()\n",
    "    min_val = df[col].min()\n",
    "    return (df[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "# Normalize all specified variables\n",
    "for var in variables:\n",
    "    df_cp[var] = lin_norm(df_cp, var)\n",
    "\n",
    "# Display the first few rows of the normalized dataframe\n",
    "df_cp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "\n",
    "def one_hot_encode(df):\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    #Initialize OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    # Apply one-hot encoding to the categorical columns\n",
    "    one_hot_encoded = encoder.fit_transform(df[categorical_columns])\n",
    "    #Create a DataFrame with the one-hot encoded columns\n",
    "    #We use get_feature_names_out() to get the column names for the encoded data\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "    # Concatenate the one-hot encoded dataframe with the original dataframe\n",
    "    df_encoded = pd.concat([df, one_hot_df], axis=1)\n",
    "    # Drop the original categorical columns\n",
    "    df_encoded = df_encoded.drop(categorical_columns, axis=1)\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "\n",
    "# # 3.1 Apply Sklearn's Affinity Propagation clustering. Visualize the created clusters\n",
    "df_cp_affinity = df.copy()\n",
    "df_cp_affinity.dropna(inplace=True)\n",
    "\n",
    "df_cp_affinity = one_hot_encode(df_cp_affinity)\n",
    "\n",
    "for var in variables:\n",
    "    df_cp_affinity[var] = lin_norm(df_cp_affinity, var)\n",
    "\n",
    "\n",
    "affinity_propagation = AffinityPropagation()\n",
    "df_cp_affinity['cluster'] = affinity_propagation.fit_predict(df_cp_affinity)\n",
    "\n",
    "\n",
    "fig = px.scatter(df_cp_affinity, x=df_cp.columns[0], y=df_cp_affinity.columns[1], color='cluster', title='Affinity Propagation Clustering')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# 3.2  Apply Sklearn's DBSCAN clustering. Visualize the created clusters\n",
    "\n",
    "df_cp_DB = df.copy()\n",
    "df_cp_DB.dropna(inplace=True)\n",
    "\n",
    "# df_sample = df_cp.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# df_sample = pd.get_dummies(df_sample, columns=['Month', 'VisitorType', 'Weekend', 'Revenue'])\n",
    "df_cp_DB = pd.get_dummies(df_cp_DB, columns=['Month', 'VisitorType', 'Weekend', 'Revenue'])\n",
    "\n",
    "\n",
    "for var in variables:\n",
    "    df_cp_DB[var] = lin_norm(df_cp_DB, var)\n",
    "\n",
    "\n",
    "# EPS dist, min_samples = min points in a cluster\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
    "# df_sample['cluster'] = dbscan.fit_predict(df_sample)\n",
    "df_cp_DB['cluster'] = dbscan.fit_predict(df_cp_DB)\n",
    "\n",
    "fig = px.scatter(df_cp_DB, x=df_cp.columns[0], y=df_cp_DB.columns[5], color='cluster', title='DBSCAN Clustering')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# 3.3   Apply Sklearn's Birch clustering. Visualize the created clusters\n",
    "\n",
    "df_cp_Birch = df.copy()\n",
    "df_cp_Birch.dropna(inplace=True)\n",
    "\n",
    "# df_sample = df_cp.sample(frac=0.1, random_state=42)\n",
    "\n",
    "df_cp_Birch = one_hot_encode(df_cp_Birch)\n",
    "\n",
    "for var in variables:\n",
    "    df_cp_Birch[var] = lin_norm(df_cp_Birch, var)\n",
    "\n",
    "birch = Birch(threshold=0.5, n_clusters=5)\n",
    "df_cp_Birch['cluster'] = birch.fit_predict(df_cp_Birch)\n",
    "\n",
    "fig = px.scatter(df_cp_Birch, x=df_cp_Birch.columns[0], y=df_cp_Birch.columns[5], color='cluster', title='Birch Clustering')\n",
    "fig.show()\n",
    "\n",
    "df_cp_Birch.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 \n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    sum = 0\n",
    "    for i in range(len(x)):\n",
    "        sum += (x[i] - y[i]) ** 2\n",
    "    return math.sqrt(sum)\n",
    "\n",
    "\n",
    "\n",
    "# 5.2\n",
    "def manhatten_distance(x, y):\n",
    "    sum = 0\n",
    "    for i in range(len(x)):\n",
    "        sum += abs(x[i] - y[i])\n",
    "    return sum\n",
    "\n",
    "\n",
    "# 5.3\n",
    "def cosine_similarity(x, y):\n",
    "    top = 0\n",
    "    x_summed = 0\n",
    "    y_summed = 0\n",
    "    for i in range(len(x)):\n",
    "        top += x[i] * y[i]\n",
    "        x_summed += x[i] ** 2\n",
    "        y_summed += y[i] ** 2\n",
    "    return top / (math.sqrt(x_summed) * math.sqrt(y_summed))\n",
    "\n",
    "# 5.4\n",
    "euclidean_output = euclidean_distance(x,y)\n",
    "manhatten_output = manhatten_distance(x,y)\n",
    "cosine_output = cosine_similarity(x,y)\n",
    "\n",
    "# Evaluate the effect which the different distance functions have on the DBSCAN algorithm. \n",
    "# To evaluate, use any of the evaluation methods of the previous task. Visualize the results to \n",
    "# support your arguments\n",
    "\n",
    "df_cp_euclidean_DB = df.copy()\n",
    "df_cp_euclidean_DB.dropna(inplace=True)\n",
    "\n",
    "df_cp_euclidean_DB = one_hot_encode(df_cp)\n",
    "\n",
    "for var in variables:\n",
    "    df_cp_euclidean_DB[var] = lin_norm(df_cp_euclidean_DB, var)\n",
    "\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10, metric=euclidean_distance)\n",
    "df_cp_euclidean_DB['cluster'] = dbscan.fit_predict(df_cp_euclidean_DB)\n",
    "\n",
    "fig = px.scatter(df_cp_euclidean_DB, x=df_cp_euclidean_DB.columns[0], y=df_cp_euclidean_DB.columns[5], color='cluster', title='DBSCAN Clustering with Euclidean Distance')\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dat01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
